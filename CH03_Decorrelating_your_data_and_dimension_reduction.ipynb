{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH03_Decorrelating_your_data_and_dimension_reduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/Unsupervised-learning-in-python/blob/master/CH03_Decorrelating_your_data_and_dimension_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjMUGQSbfsCJ",
        "colab_type": "text"
      },
      "source": [
        "#Dimension reduction\n",
        "\n",
        "* compresses data\n",
        "* more efficient storage and computation\n",
        "* can remove \"noise\" and unimportant features\n",
        "\n",
        "#Principal Component Analysis (PCA)\n",
        "\n",
        "* fundamental dimention reduction technique\n",
        "> * decorrelation\n",
        "> * dimension reduction\n",
        "* rotates data to be aligned with axis (mean=0)\n",
        "\n",
        "* PCA is scikit-learn component like KMeans() or StandardScaler()\n",
        "* can use fit() and transform() separately \n",
        "* (transform can be applied to new data)\n",
        "\n",
        "#PCA features\n",
        "\n",
        "* rows represent samples\n",
        "* columns represent features\n",
        "\n",
        "#Pearson Correlation\n",
        "\n",
        "* measures linear correlations from -1 to 1 \n",
        "* zero = no correlation\n",
        "\n",
        "#principle components\n",
        "\n",
        "* meaning direction of variance\n",
        "* available as components_\n",
        "* rows of components_ defines displacement from mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJf9qXesfnYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:,0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:,1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snDVU2q6lA40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXAW2DrqpfuA",
        "colab_type": "text"
      },
      "source": [
        "#Intrinsic Dimension\n",
        "\n",
        "* intrinsic dimension = min dim to approximate a dataset\n",
        "* intrinsic dim = # of PCA significant features\n",
        "* can vary depending on the cutoff for significance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJpwzHXDlAwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0zu37hulAoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EoCYwdOwvhY",
        "colab_type": "text"
      },
      "source": [
        "#Dimension reduction with PCA\n",
        "\n",
        "* must identify how many dimensions you want to keep\n",
        "* good choice = intrinsic dimension\n",
        "\n",
        "#Word Frequency Array\n",
        "\n",
        "* rows represent documents\n",
        "* columns represent words\n",
        "* typically sparse so use csr matrix\n",
        "* saves space by remembering only non-zero values\n",
        "* cannot use PCA with Word Freq Array\n",
        "* have to use TruncatedSVD from sklearn instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smN364sOy8El",
        "colab_type": "text"
      },
      "source": [
        "#dimension reduction example here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q6ulPihlAbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA model with 2 components: pca\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA instance to the scaled samples\n",
        "pca.fit(scaled_samples)\n",
        "\n",
        "# Transform the scaled samples: pca_features\n",
        "pca_features = pca.transform(scaled_samples)\n",
        "\n",
        "# Print the shape of pca_features\n",
        "print(pca_features.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6fyD8WIy3AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer: tfidf\n",
        "tfidf = TfidfVectorizer() \n",
        "\n",
        "# Apply fit_transform to document: csr_mat\n",
        "csr_mat = tfidf.fit_transform(documents)\n",
        "\n",
        "# Print result of toarray() method\n",
        "print(csr_mat.toarray())\n",
        "\n",
        "# Get the words: words\n",
        "words = tfidf.get_feature_names()\n",
        "\n",
        "# Print words\n",
        "print(words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nimDbEROy2wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a TruncatedSVD instance: svd\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "\n",
        "# Create a KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=6)\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(svd, kmeans)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az48-V041082",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}